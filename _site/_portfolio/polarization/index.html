<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Polarization - Joe Koch</title>
<meta name="description" content="A simulation of polarization in the scientific community.">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Joe Koch">
<meta property="og:title" content="Polarization">
<meta property="og:url" content="http://localhost:4000/_portfolio/polarization/">


  <meta property="og:description" content="A simulation of polarization in the scientific community.">



  <meta property="og:image" content="http://localhost:4000/assets/images/polarization/large25.gif">









  

  


<link rel="canonical" href="http://localhost:4000/_portfolio/polarization/">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Joe Koch",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Joe Koch Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">Joe Koch</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://joe-koch.github.io/personal/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/assets/koch-resume.pdf" >Résumé</a>
            </li><li class="masthead__menu-item">
              <a href="/about/" >About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/portfolio/" >Projects</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style=" background-image: url('/assets/images/polarization/large25.gif');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Polarization

        
      </h1>
      
        <p class="page__lead">A simulation of polarization in the scientific community.
</p>
      
      
      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/redwood-selfie.jpg" alt="Joe Koch" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Joe Koch</h3>
    
    
      <p class="author__bio" itemprop="description">
        Machine Learning / Data Science / Storytelling
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://www.https://www.linkedin.com/in/joe-koch-76431063/" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://github.com/Joe-Koch" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="mailto:kochzoe@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
      
      
      <p><a href="https://github.com/Joe-Koch/polarization/blob/master/fastpolarize.m">View the project’s code</a></p>

      
    
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Polarization">
    <meta itemprop="description" content="A simulation of polarization in the scientific community.">
    
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu">
  <li><a href="#simulating-ideological-polarization">Simulating Ideological Polarization</a>
    <ul>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#implementation">Implementation</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#conclusions">Conclusions</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </li>
</ul>
            </nav>
          </aside>
        
        <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async=""></script>

<h1 id="simulating-ideological-polarization">Simulating Ideological Polarization</h1>

<h2 id="introduction">Introduction</h2>

<p>How does ideological polarization, where subgroups within a society maintain stable opposing beliefs, arise? This agent-based simulation demonstrates a mechanism by which a group of learners who share evidence, and who have the same aims and values, can nonetheless become polarized. The model is based on the framework proposed in Scientific Polarization<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, and explores how the altering the size of one’s cohort affects rates of polarization and convergence on the correct belief.</p>

<h2 id="implementation">Implementation</h2>

<p class="notice"><strong>tl;dr:</strong> Each scientist is trying to determine which action is better, A (an acceptable choice) or B (a better choice). If they believe action B is better, they’ll create more evidence about action B. They’ll look at the evidence of their nearest neighbors, as well, but will believe that the evidence is false if they deem their neighbor an untrustworthy source. Trustworthiness is partially based on how similar their beliefs were to begin with. They then update their beliefs based on the new evidence and their perceived probability that the evidence is true. The simulation continues until the beliefs are stable, ending either in community consensus or polarization.</p>

<p>We start with the community of scientists arranged in a square grid, where each agent in the community has some uniformly distributed random “credence” between 0 and 1 that action B is better than action A. The success rate of action A is known to be .5, while that of action B is unknown to the scientists. We choose some probability of success for action B, called <script type="math/tex">P_B</script>, between .5001 and .8. At each timestep, each scientist who believes action B is better than action A performs an experiment where they try action B <script type="math/tex">n</script> times and count the successes (a Bernoulli trial).</p>

<p>The scientists also each have an unchanging neighborhood of size <script type="math/tex">degree</script> comprised of the scientists nearest them. Each scientist then updates their credence that B is better than A based on the new evidence of any scientists in their neighborhood. This update is modeled using the equation given in the original paper, and uses a combination of Bayes’ rule, Jeffrey conditionalization, and the premise that scientists may disbelieve in the experimental results of scientists they perceive to be untrustworthy sources.</p>

<p class="notice"> <b>The updates, in mathematical detail:</b> 
  <br /> Suppose Jill is looking at Ian’s evidence and determining how it should affect her beliefs that action B is better than action A. Jill might not fully trust Ian’s credibility, meaning she has credence $P_f(E) \leq 1$ that his evidence is false. Under Jeffrey conditionalization Jill will update her beliefs, in light of Ian’s evidence, using the following formula: $$P_f(H) = P_i(H|E) \cdot P_f (E) + P_i(H \vert ~ E) \cdot P_f (~ E)$$ This says that Jill's final belief in the hypothesis that B is better than A is equal to her credence that the evidence is real, $P_f (E)$, multiplied by the belief she would obtain via strict conditionalization on that evidence, $P_i(H | E)$, plus her credence that the evidence did not occur, $P_f (~ E)$, multiplied by the belief she would have by strict conditionalization if it had not occurred, $P_i(H|~ E)$. Here, the strict conditionalization updates are given by Bayes' rule: 
  $$P_i(H|E) = \frac{P_i(E|H) \cdot P_i(H)}{P(E)}$$ 
  The formula for how much Jill trusts Ian's evidence is given by 
  $$P_f (E) =  \max(1 - d \cdot m \cdot (1 - P_i(E), 0)$$ 
  Where $d$ is the distance between Jill's and Ian's credences in the hypothesis, $m$ is a multiplier that captures how quickly agents become uncertain about the evidence of their peers as their beliefs diverge, and $P_i(E)$ is Jill’s initial probability of the evidence occurring given her credence in the hypothesis. This formula, as described in the original paper, is fairly arbitrary. The important thing is that as the distance between Jill's and Ian's beliefs grows, Jill finds Ian's evidence less credible. She then may update her beliefs away from what Ian's evidence suggests.
</p>

<p>Once the scientists’ credences are stable or the pre-set maximum number of timesteps have occurred, the simulation terminates. A stable outcome is one in which every agent either (a) has credence &gt; .99 that action B is correct or else (b) has credence low enough that, even upon seeing evidence that action B is better, they will disregard the evidence. The community may end up in a state of correct consensus (all scientists believe action B is better than action A), incorrect consensus (all scientists believe A is better than B), or polarization.</p>

<figure class="">
  <img src="/assets/images/polarization/example-run.gif" alt="" />
  
    <figcaption>An example of the simulation.
</figcaption>
  
</figure>

<p><strong>Code optimization:</strong> Initially, the simulation ran quite slowly. The calculation of how much to update one’s credence based on a given neighbor’s evidence is computationally intensive. However, since the function is only dependent on the scientists’ starting credence, their neighbor’s credence, and their neighbor’s evidence (represented by how many successes they saw in <em>n</em> trials), I was able to save these “update values” in a matrix so they’ll only need to be calculated once at the beginning of the simulation. Credences were discretretized by separating them into 20 bins. These update values are persistent, so even if you run the simulation multiple times, they’ll only need to be recalculated if the parameters are changed. These improvements reduced CPU time by about a factor of 10.</p>

<h2 id="results">Results</h2>

<p>Let’s take a look at how varying the parameters of the model affected outcomes.</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Definition</th>
      <th>Range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">P_B</script></td>
      <td>The probability that action B succeeds. This is compared to that of action A, which has <script type="math/tex">P_A</script>=.5.</td>
      <td>.5001-.8</td>
    </tr>
    <tr>
      <td><script type="math/tex">m</script></td>
      <td>The multiplier that determines how quickly scientists begin to mistrust those with different beliefs.</td>
      <td>1-3</td>
    </tr>
    <tr>
      <td><script type="math/tex">degree</script></td>
      <td>The size of each scientist’s neighborhood. A measure of community connectedness.</td>
      <td>1-All Scientists</td>
    </tr>
    <tr>
      <td><script type="math/tex">n</script></td>
      <td>The number of tests each scientists runs in every round of their experiments.</td>
      <td>1-100</td>
    </tr>
    <tr>
      <td><script type="math/tex">dim</script></td>
      <td><script type="math/tex">dim^2</script> is the number of scientists in the community.</td>
      <td>1-10</td>
    </tr>
  </tbody>
</table>

<p>Increasing the size of the scientists’ neighborhoods increases the probability that any given scientist will converge to the correct belief. Increasing the distance between $P_B$, the probability of success of the better action B, and $P_A$, the probability of success of the acceptable action A, also predictably leads to more scientists finding action B is better. Increasing $m$, however, decreases the proportion of correct individuals, as scientists more strongly distrust the evidence of others with dissimilar beliefs.</p>

<figure class="">
  <img src="/assets/images/polarization/CvsDvsPvsM.svg" alt="" />
  
    <figcaption>Simulations had $dim=4$, $n$=20. For each graph, the other parameters were uniformly sampled over their range.
</figcaption>
  
</figure>

<p>The $degree$ and $P_B$ parameters don’t strongly interact. However, since $m$ determines how scientists react to the evidence of others, its effect is less pronounced when the degree is smaller and scientists have fewer neighbors.</p>

<figure class="">
  <img src="/assets/images/polarization/CvsDP.svg" alt="" />
  
    <figcaption>Averaged over 50 trials with $m = 1.5$, $dim=4$, $n$=20.
</figcaption>
  
</figure>

<figure class="">
  <img src="/assets/images/polarization/CvsDM.svg" alt="" />
  
    <figcaption>Averaged over 50 trials with $P_B = .8$, $dim=4$, $n$=20.
</figcaption>
  
</figure>

<p>Adjusting the parameters also affects the probability that a community will reach consensus or polarize. Increasing $degree$ leads to more and community consensus to the right answer as well as slightly more polarization. Notice that consensus on the wrong answer is sometimes more common than consensus on the right answer. Remember that in this simulation, those who believe, incorrectly, that action A outperforms action B will no longer perform their own experiments to collect evidence on action B. Although no longer investing time and energy to investigating a “worse” option is generally a rational thing to do, it still allows for consensus on the incorrect answer to arise more easily.</p>

<figure class="">
  <img src="/assets/images/polarization/statevsD.svg" alt="" />
  
</figure>

<p>Again, we see that varying $m$ is not as significant when $degree$ is small. $P_B$ is also more significant when $degree$ is small, and communities are more susceptible to consensus to the wrong answer.</p>

<figure class="">
  <img src="/assets/images/polarization/polarvsDP.svg" alt="" />
  
</figure>

<figure class="">
  <img src="/assets/images/polarization/polarvsDM.svg" alt="" />
  
</figure>

<p>Did scientists tend to form in close-knit pockets, or were they well-dispersed over the space? We can measure how blended a group of scientists is by checking the entropy. The entropy of a purely random initial map will be around 3. Here we see that, over time, entropy decreases and the communities become more homogenous. It’s only when $degree=1$, when scientists are not considering the evidence of their neighbors at all, that entropy actually increases as the scientists become more firmly set in their beliefs.</p>
<figure class="">
  <img src="/assets/images/polarization/entropyvsT.svg" alt="" />
  
</figure>

<h2 id="conclusions">Conclusions</h2>

<p>Discarding evidence of those deemed to be untrustworthy sources can be a mechanism for the emergence of polarization. In these simulations, considering the evidence of others led to greater probabilities of believing in the correct answer, with benefits tapering off significantly after 7 or more neighbors’ evidence was considered.</p>

<h2 id="references">References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>O’Connor, Cailin &amp; Weatherall, James Owen (2017). Scientific polarization. <em>European Journal for Philosophy of Science</em> 8 (3):855-875. https://philpapers.org/rec/OCOSP <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#computational-modeling" class="page__taxonomy-item" rel="tag">computational modeling</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#matlab" class="page__taxonomy-item" rel="tag">MATLAB</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#simulation" class="page__taxonomy-item" rel="tag">simulation</a>
    
    </span>
  </p>




        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>



    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://www.https://www.linkedin.com/in/joe-koch-76431063/" rel="nofollow noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://github.com/Joe-Koch" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="mailto:kochzoe@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Joe Koch. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script data-search-pseudo-elements defer src="https://use.fontawesome.com/releases/v5.7.1/js/all.js" integrity="sha384-eVEQC9zshBn0rFj4+TU78eNA19HMNigMviK/PU/FFjLXqa/GKPgX58rvt5Z8PLs7" crossorigin="anonymous"></script>








  </body>
</html>
